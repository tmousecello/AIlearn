# AI筆記

## AI GO 的課
### 機器學習
映射的概念  
把一個資料變成結果  
方法:監督式學習、非監督式學習  
語音->文字(語音識別)  
人臉->身分(人臉辨識)  
計算力重點是GPU  
資料須注意法規，例如歐盟的個人隱私法案  
奧卡姆剃刀原則:相同效果的前提下，模型越簡單越好  
要多注意新的技術發展  
要清楚問題  

### 神經網路
參考人類的大腦  
人腦:樹突收集訊號->神經核匯總轉換->傳遞訊號  
捲積神經網路:模仿人類視網膜(多用於判別圖片 可作中文分析)  
遞迴神經網路:模仿人類記憶(多用於語言建模 缺點:無法平行運算)  
強化學習:模仿人類適應能力(結果制，做對給獎勵，做錯給處罰 紅蘿蔔+棍子)  
大腦的稀疏姓:大腦面對刺激一次只會觸發一小部分的腦  
心理的距離跟數學上的距離不一樣  
人類的大腦活化函數是S型，中間那段才是線性關係，對極值較無感  

### 深度學習
重點是表徵學習  
讓機器以更像人腦的方式思考  
多層抽取特徵
特徵->規則->預測  
機器可以把人類無法想像的規則找出，不受人的思考制約  
會混入一點髒數據  
向量方式表現(所以G0PU算起來才強)  
多維壓縮至低維,找出最近的位置  
微分、向量、矩陣運算  
機器較難判斷模糊的東西  
人腦可以建立多種規則(特徵點)  
機器自己找出多種特徵、規則  
奧卡姆剃刀原則不一定成立  
Denoise Auto-Encoder:  
搞髒的圖->變成乾淨的圖  
機器學會分辨正確訊息跟噪音  
模仿人腦的S曲線，思考會更接近人

### 優點 
機器學習過的一定會記得  
機器可以在短時間內處理大量資料  


### 缺點
機器現在不會真的思考  
機器不懂常識，只懂得輸入輸出間的關聯  
機器很難處理特例  
機器需要明確結果與"標準化"輸入才能運作  
AI產品不能放在外面讓它不斷自己學習，不然很可能會學壞    
一個運算模型一次只能做一件事  
無法自由切換切入問題角度，切換模型需要人類定義解決方法  
AI無法做沒有標準答案的判斷  
容易學壞(ex:性別、種族歧視)  

### 機器視覺
希望建立(機器可讀)像素訊號與人類視覺的映射關係  
人看到的是狗，機器看到的是一堆數據  
通常尺寸是224*224  
輸入需要嚴格定義與固定  
需要基於標註(標註需標準化)  
需要一點一點的累積  
標註很重要  
把人類對世界的了解透過標註讓機器理解  

### 對抗生成
Generative Adversarial Network,GAN  
透過彼此競爭互相優化  
一個模型負責製造假照片  
另一個負責辨識  
再用強化學習(獎勵&懲罰)  
機器容易被騙(對抗式攻擊(資安部分))   

### 應用
AI可以進行藝術設計(找尋最相關結構進行風格轉移)  
可以識別服裝並且了解消費習慣   
可以用一些人無法辨識的信號進行處理  
ex:用wifi信號強弱判定牆後人的行動  
GAN可以做美容軟體  
馬賽克還原  
自然語言理解  

### 訓練數據
訓練->推理
數據要小心規則與巧合  
定義要清楚  
所以訓練數據跟測試數據要分開  
測試數據要分時間內測試和時間外測試(訓練結果驗證新的數據)  
訓練需要大量算力(GPU&TPU)  
邊緣運算-由於推理不像訓練需要大量算力，讓推理在邊緣端(效能較低裝置或用戶端?)處理  
欠擬合(underfitting):模型效度太低；過擬合(overfitting):模型效度太好，機器背下答案  
過擬合可能很難分辨，造成機器學好的假象  
萬能建模定理:神經網路中，只要一層足夠量的隱藏層，就可以模擬任何函數  
數據量不夠多，機器就會背答案  
需要以另一組數據檢視是否過擬合  
使用數據增強(搞髒數據):稍微改變數據樣式  EX:灰度變化、水平翻轉、旋轉、裁切、調色  
人類的視覺空間可交換性:一隻貓不會從地板跑到屋頂上，人就把它看成一隻狗  
訓練機器從缺失的數據判斷  
學好open cv  
數據要正規化(可能標準化?)  
深度學習多以0為中間(數據減掉平均值)  
RGB的正規化通常是減127.5(256的一半)再除以127.5(把數據範圍縮小到-1到1之間)  
把數據整成常態分布，會接近人腦的S曲線微分後的樣子  
但真實世界大多不是常態分布  

### 數據標註
辨識模型標註方法一:把各類數據放在對應資料夾  
標註方法:LabelMe(標籤框選位置)  
多用打方框，因標註容易  
框要在四個極端點  
要避免機器誤解，需要清楚線索  
人臉規範是68個關鍵點  


### 張量
scalar-標量:純數值  
vector-向量:一維數據  
matirx-矩陣:二維數據  
tensor-張量:N維數據(除了scalar)  
在深度學習的三維空間表示:C(通道數)*H(高)*W(寬)，高一定在寬前面(按字母順序)  
tensor flow比較特別，是H*W*C  
png是四維(要加上透明度)  
顏色排列:tensorflow是RGB(按人類習慣)，opencv是BGR(按字母順序)  
顏色越淺，數字越大(以光強度來理解)  
tensorflow會比較慢  
tensorflow的標準比較特別，要注意
python套件讀取圖片需要注意RGB、HWC的問題  

### 缺失數據
缺失數據處理重要性逐漸降低  
因為用問卷問的資料不一定正確，人可能因面子或其他原因撒謊，或實際使用的不是填問卷的人  
現在以行為數據居多  
圖片若太小，可以外圍用0填滿，或縮放
以用0或隨機亂數滿足張量的固定大小
深度學習可能會人為刻意製造缺失數據(髒數據)

### 遷移學習
站在大佬的肩膀上  
預訓練過的機器比沒訓練過的機器更快  
灌輸機器嘗試的概念(讓機器了解通則)  
預訓練要有龐大數據  
imagenet有預訓練模型  
pytorch可用torchvision  
其他可以用onnx   
保留前面權重，處理後面連接層  
可以用較少數據訓練  
如果數據規模較多，可以更改較近的權重  
如果數據規模極大，可以優化較前面的權重  
數據量小，條的範圍小  

## asus的課

### 描述AI的對照
人類怎麼學新知識->電腦就怎麼學(回歸與分類)  
如何以經驗導出結論(邏輯、推理)  
如何以經驗處理新事物的刺激  

### 訓練階段
大量資料->找出特徵、給予標籤(標註)->訓練模型  
可利用已有的model改良  
遊戲截圖->標註距離->得到結果  
可以用雲端服務訓練  
teachablemachine.withgoogle.com 可體驗  

### 推論階段
由之前訓練的資料判斷，產出結果  
ex:人臉識別(即時運算)  
市面上有推論用的加速卡  
可能有現成的AI模型  

## 鴻海的課

### 把問題化為函數
前面說的映射


















